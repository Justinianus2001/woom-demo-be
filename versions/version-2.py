import subprocess
import os
import tempfile
import librosa
from pydub import AudioSegment
import numpy as np
from scipy import signal
import soundfile as sf
import torch
import torchaudio  # Sá»­ dá»¥ng Ä‘á»ƒ noise reduction náº¿u cÃ³ model, nhÆ°ng fallback HPSS

def calculate_duration_from_analysis(picked_audio):
    """PhÃ¢n tÃ­ch file Ä‘á»ƒ láº¥y duration chÃ­nh xÃ¡c cho 4 nhá»‹p tim (dÃ¹ng Librosa)."""
    try:
        y, sr = librosa.load(picked_audio, sr=None)
        tempo, beats = librosa.beat.beat_track(y=y, sr=sr)
        if len(beats) >= 5:
            duration = librosa.frames_to_time(beats[4] - beats[0], sr=sr)
            return duration
    except Exception as e:
        print(f"âŒ PhÃ¢n tÃ­ch tháº¥t báº¡i: {e}")
    return None

def get_mean_volume(audio_path):
    """Äo mean volume (dBFS) dÃ¹ng PyDub."""
    try:
        audio = AudioSegment.from_file(audio_path)
        return audio.dBFS
    except Exception as e:
        print(f"âŒ Äo volume tháº¥t báº¡i: {e}")
        return -16.0

def run_ffmpeg(command):
    """Cháº¡y FFmpeg command vÃ  check success."""
    process = subprocess.run(command, shell=True, capture_output=True, text=True)
    if process.returncode != 0:
        print(f"âŒ FFmpeg failed: {process.stderr}")
        return False
    return True

def apply_noise_reduction(y, sr):
    """Cáº£i tiáº¿n: Sá»­ dá»¥ng HPSS tá»« Librosa Ä‘á»ƒ tÃ¡ch percussive (nhá»‹p tim) tá»« harmonic (noise nÆ°á»›c á»‘i)."""
    y_harmonic, y_percussive = librosa.effects.hpss(y)
    return y_percussive  # Giá»¯ percussive lÃ  nhá»‹p tim Ä‘áº­p

def match_tempo(asset_path, tempo_factor, output_path):
    """Cáº£i tiáº¿n: Time-stretch asset Ä‘á»ƒ khá»›p tempo chÃ­nh xÃ¡c dÃ¹ng Librosa."""
    y, sr = librosa.load(asset_path, sr=None)
    y_stretched = librosa.effects.time_stretch(y, rate=tempo_factor)
    sf.write(output_path, y_stretched, sr)
    return output_path

def tune_to_432hz(input_path, output_path):
    """Cáº£i tiáº¿n: Pitch shift toÃ n bá»™ audio xuá»‘ng 432Hz tuning tá»« 440Hz."""
    y, sr = librosa.load(input_path, sr=None)
    n_steps = 12 * np.log2(432 / 440)  # â‰ˆ -0.3176 semitones
    y_tuned = librosa.effects.pitch_shift(y, sr=sr, n_steps=n_steps)
    sf.write(output_path, y_tuned, sr)

def mix_audio(asset_audio, picked_audio, output_path, original_bpm=120, target_bpm=120):
    """Mix audio cáº£i tiáº¿n: HPSS khá»­ táº¡p Ã¢m, time-stretch tempo, dynamic threshold, tune to 432Hz."""
    print("ğŸ” Äang phÃ¢n tÃ­ch file Ä‘á»ƒ tÃ¬m 4 nhá»‹p tim chÃ­nh xÃ¡c...")
    tempo_factor = original_bpm / target_bpm
    analyzed_duration = calculate_duration_from_analysis(picked_audio)

    if analyzed_duration is not None:
        duration_seconds = analyzed_duration + 0.5  # ThÃªm buffer
        print(f"âœ… PHÃ‚N TÃCH THÃ€NH CÃ”NG: Cáº¯t chÃ­nh xÃ¡c 4 nhá»‹p = {duration_seconds:.3f}s")
    else:
        duration_seconds = 4 * (60.0 / original_bpm) + 0.5
        print(f"âš ï¸ PhÃ¢n tÃ­ch tháº¥t báº¡i. DÃ¹ng cÃ´ng thá»©c chuáº©n 4 nhá»‹p/BPM: {duration_seconds:.3f}s")

    print(f"ğŸ“Š Tempo factor: {tempo_factor}")

    with tempfile.TemporaryDirectory() as temp_dir:
        temp_wav_path = os.path.join(temp_dir, 'picked_temp.wav')
        denoised_path = os.path.join(temp_dir, 'picked_denoised.wav')
        silenced_path = os.path.join(temp_dir, 'picked_silenced.wav')
        trimmed_path = os.path.join(temp_dir, 'picked_trimmed.wav')
        normalized_picked_path = os.path.join(temp_dir, 'picked_normalized.wav')
        stretched_asset_path = os.path.join(temp_dir, 'asset_stretched.wav')
        normalized_asset_path = os.path.join(temp_dir, 'asset_normalized.wav')
        mixed_temp_path = os.path.join(temp_dir, 'mixed_temp.mp3')
        tuned_output_path = output_path  # Final tuned

        # BÆ°á»›c 1: Chuyá»ƒn Ä‘á»•i picked sang WAV
        print("ğŸ”„ BÆ°á»›c 1: Chuyá»ƒn Ä‘á»•i sang WAV...")
        convert_cmd = f'ffmpeg -y -i "{picked_audio}" -ac 2 -ar 44100 "{temp_wav_path}"'
        if not run_ffmpeg(convert_cmd):
            return

        # BÆ°á»›c 2.1: Khá»­ táº¡p Ã¢m dÃ¹ng HPSS
        print("ğŸ”Š BÆ°á»›c 2.1: Khá»­ táº¡p Ã¢m (HPSS tÃ¡ch nhá»‹p tim tá»« noise nÆ°á»›c á»‘i)...")
        y, sr = sf.read(temp_wav_path)
        if y.ndim > 1:
            y = np.mean(y, axis=1)  # Mono for HPSS
        y_denoised = apply_noise_reduction(y, sr)
        sf.write(denoised_path, y_denoised, sr)

        # BÆ°á»›c 2.2: Loáº¡i bá» khoáº£ng láº·ng (dynamic threshold)
        print("ğŸ”Š BÆ°á»›c 2.2: Loáº¡i bá» khoáº£ng láº·ng Ä‘áº§u (dynamic threshold)...")
        peak_db = librosa.amplitude_to_db(np.max(np.abs(y_denoised)))
        threshold_db = max(-50, peak_db - 30)
        print(f"Dynamic threshold: {threshold_db}dB")
        silence_cmd = (
            f'ffmpeg -y -i "{denoised_path}" '
            f'-af silenceremove=start_periods=1:start_duration=0:start_threshold={threshold_db}dB:detection=peak '
            f'"{silenced_path}"'
        )
        if not run_ffmpeg(silence_cmd):
            return

        # BÆ°á»›c 2.3: Cáº¯t 4 nhá»‹p
        print("ğŸ”Š BÆ°á»›c 2.3: Cáº¯t Ä‘Ãºng 4+ nhá»‹p Ä‘á»ƒ giá»¯ Ä‘áº§y Ä‘á»§...")
        trim_cmd = f'ffmpeg -y -i "{silenced_path}" -t {duration_seconds} "{trimmed_path}"'
        if not run_ffmpeg(trim_cmd):
            return

        if os.path.getsize(trimmed_path) == 0:
            print("âŒ Trimmed file empty, fallback to no silence remove.")
            fallback_trim_cmd = f'ffmpeg -y -i "{denoised_path}" -t {duration_seconds} "{trimmed_path}"'
            run_ffmpeg(fallback_trim_cmd)

        # BÆ°á»›c 2.4: Chuáº©n hÃ³a picked
        print("ğŸ”Š BÆ°á»›c 2.4: Chuáº©n hÃ³a Ã¢m lÆ°á»£ng picked...")
        picked_seg = AudioSegment.from_file(trimmed_path)
        picked_seg = picked_seg.normalize()
        if picked_seg.dBFS < -20:
            print("âš ï¸ Volume tháº¥p, boost +6dB.")
            picked_seg += 6
        picked_seg.export(normalized_picked_path, format="wav")

        # BÆ°á»›c 3: Äá»“ng bá»™ tempo asset vÃ  chuáº©n hÃ³a
        print("ğŸ”Š BÆ°á»›c 3: Äá»“ng bá»™ tempo asset vÃ  chuáº©n hÃ³a...")
        match_tempo(asset_audio, tempo_factor, stretched_asset_path)
        normalize_asset_cmd = (
            f'ffmpeg -y -i "{stretched_asset_path}" -ar 44100 -ac 2 '
            f'-af loudnorm=I=-16:TP=-1.5:LRA=11 "{normalized_asset_path}"'
        )
        if not run_ffmpeg(normalize_asset_cmd):
            return

        # BÆ°á»›c 4: Mix (tá»‰ lá»‡ 0.8:0.2 Ä‘á»ƒ tim thai lÃ m ná»n, bá» reverb Ä‘á»ƒ trÃ¡nh lá»—i)
        print("ğŸµ BÆ°á»›c 4: Mix audio (Tá»‰ lá»‡ 0.8:0.2 Ä‘á»ƒ tim thai lÃ m ná»n)...")
        vol_asset = get_mean_volume(normalized_asset_path)
        vol_picked = get_mean_volume(normalized_picked_path)
        print(f"ğŸ“Š Post-norm Volumes -> Asset: {vol_asset} dB, Picked: {vol_picked} dB")

        diff = vol_asset - vol_picked
        asset_filter = f"[0:a]volume={max(0, -diff)}dB[a0];"
        picked_filter = f"[1:a]volume={max(0, diff)}dB,aloop=loop=-1:size=2e+09[a1];"

        mix_cmd = (
            f'ffmpeg -y -i "{normalized_asset_path}" -i "{normalized_picked_path}" '
            f'-filter_complex "{asset_filter}{picked_filter}[a0][a1]amix=inputs=2:duration=first:dropout_transition=3:weights=0.8 0.2[a]" '
            f'-map "[a]" -c:a libmp3lame -q:a 2 "{mixed_temp_path}"'
        )
        if run_ffmpeg(mix_cmd):
            print(f"âœ… Mixing successful! Tuning to 432Hz...")
            tune_to_432hz(mixed_temp_path, tuned_output_path)
            print(f"âœ… Tuned output saved at {output_path}")
        else:
            print("âŒ Mixing failed")

# Usage example
mix_audio("twinkle_star.mp3", "Heartbeat5_bpm140.wav", "demo_version_2.mp3")